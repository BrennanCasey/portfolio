{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.select import Select\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.select import Select\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Reviews = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pageUrl = 'https://www.amazon.com/RockBirds-Flashlights-Bright-Aluminum-Flashlight/product-reviews/B00X61AJYM/ref=cm_cr_arp_d_paging_btm_1?sortBy=recent&pageNumber=1&reviewerType=avp_only_reviews&formatType=all_formats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path='./chromedriver 75')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "authors = []\n",
    "ratings = []\n",
    "titles = []\n",
    "texts = []\n",
    "dates = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Used to check length of every list to make sure dataframe can be made "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(authors))\n",
    "print(len(ratings2))\n",
    "print(len(titles))\n",
    "print(len(texts))\n",
    "print(len(cleanText))\n",
    "print(len(dates))\n",
    "print(len(pos_ScoreOfText))\n",
    "print(len(neg_ScoreOfText))\n",
    "print(len(pos_ScoreOfTitle))\n",
    "print(len(neg_ScoreOfTitle))\n",
    "print(len(goodWordsInSent))\n",
    "print(len(badWordsInSent))\n",
    "print(len(isGuest))\n",
    "print(len(lengthOfText))\n",
    "print(len(wordsInAuthor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page Scraper. -Takes Review Text, Title, Author Name, Date, and Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pageQuery():\n",
    "\n",
    "    driver.get(pageUrl)\n",
    "    n = 0\n",
    "    running = True\n",
    "    #wait\n",
    "    time.sleep(5)\n",
    "    #wait-finished\n",
    "    while n <= 1300:\n",
    "\n",
    "        wrapper = driver.find_element_by_id('cm_cr-review_list')\n",
    "        elements = wrapper.find_elements_by_css_selector(\"div[data-hook='review']\")\n",
    "        for each in elements:\n",
    "            title = 'null'\n",
    "            author = 'null'\n",
    "            date = 'null'\n",
    "            text = 'null'\n",
    "            rating = 'null'\n",
    "            title = each.find_element_by_css_selector(\"a[data-hook='review-title']\").text\n",
    "            titles.append(title)\n",
    "\n",
    "            author = each.find_element_by_css_selector(\"a[data-hook='review-author']\").text\n",
    "            authors.append(author)\n",
    "\n",
    "            date = each.find_element_by_css_selector(\"span[data-hook='review-date']\").text\n",
    "            date = date.replace('on ', \"\")\n",
    "            dates.append(date)\n",
    "\n",
    "            \n",
    "\n",
    "            text = each.find_element_by_css_selector(\"span[data-hook='review-body']\").text\n",
    "            texts.append(text)\n",
    "\n",
    "            element = each.find_element_by_css_selector('a[class=\"a-link-normal\"]').get_attribute(\"title\")\n",
    "            rating = element.replace(' out of 5 starts', '')\n",
    "            ratings.append(rating)\n",
    "            \n",
    "            n+=1\n",
    "            \n",
    "            if n % 100 == 0: \n",
    "                print(str(n) + \"Reviews Collected\")\n",
    "            \n",
    "        nextPage()\n",
    "        time.sleep(5)\n",
    "        \n",
    "    print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Switches to the next page of amazon reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nextPage():\n",
    "\n",
    "        element = driver.find_element_by_css_selector(\"li[class='a-last']\")\n",
    "        nextButton = element.find_element_by_css_selector(\"a\")\n",
    "        nextButton.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100Reviews Collected\n",
      "200Reviews Collected\n",
      "300Reviews Collected\n",
      "400Reviews Collected\n",
      "500Reviews Collected\n",
      "600Reviews Collected\n",
      "700Reviews Collected\n",
      "800Reviews Collected\n",
      "900Reviews Collected\n",
      "1000Reviews Collected\n",
      "1100Reviews Collected\n",
      "1200Reviews Collected\n",
      "1300Reviews Collected\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "pageQuery()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most extra factors that are extracted defined in one place. Some are defined in their respective for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extra factors\n",
    "isGuest = []\n",
    "lengthOfText = []\n",
    "wordsInText = []\n",
    "lengthOfAuthor = []\n",
    "wordsInAuthor = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checks if user name is Amazon Customer. I figured this might be a good indicator that a review might be fake because they are not displaying a username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for each in authors:\n",
    "    if each == \"Amazon Customer\":\n",
    "        isGuest.append(1)\n",
    "    else:\n",
    "        isGuest.append(0)\n",
    "        \n",
    "    numOfWords = each.count(' ') + 1\n",
    "    lengthOfString = len(each)\n",
    "    lengthOfAuthor.append(lengthOfString)\n",
    "    wordsInAuthor.append(numOfWords)\n",
    "    \n",
    "for each in texts:\n",
    "    wordsInText.append(each.count(' ') + 1)\n",
    "    lengthOfText.append(len(each))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleanText = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removes stopwords and puntuation as well as pronouns and creates a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "for sentence in texts:\n",
    "    doc = nlp(sentence)\n",
    "    newSentence = []\n",
    "    for token in doc:\n",
    "        if token.text in cachedStopWords:\n",
    "            pass\n",
    "        elif token.is_punct:\n",
    "            pass\n",
    "        elif token.pos_ == 'PRON':\n",
    "            pass\n",
    "        elif token.lemma_ == '-PRON-':\n",
    "            pass\n",
    "        else:\n",
    "            newSentence += [token.lemma_]\n",
    "            \n",
    "            \n",
    "    cleanText.append(newSentence)\n",
    "        \n",
    "cleanText    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### common positive and negative words in reviews. This is used to make a scoreOfText feature for both positive and negative review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make a list of positive words\n",
    "#check if text contains a positive word\n",
    "goodWords = ['awesome', 'excellent', 'solid', 'perfect', 'good', 'love', 'better', 'easy', 'great', 'powerful', 'inexpensive', 'amazing', 'happy', 'terrific']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make a list of negative words\n",
    "badWords = ['poor', 'trash', 'garbage', 'broken', 'broke','terrible', 'bad', 'defective', 'dropped', 'drop', 'disappointed', 'faulty', 'miss', 'waste', 'weak', 'stopped', 'mediocre','cheaply','junk', 'china','hate',]\n",
    "#check if text contains a positive word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n"
     ]
    }
   ],
   "source": [
    "#extra all adjectives\n",
    "#combine into string with str(word)+\".a.01\"\n",
    "#store pos_score()  \n",
    "#store neg_score() and add to df\n",
    "pos_ScoreOfText = []\n",
    "neg_ScoreOfText = []\n",
    "n = 0\n",
    "\n",
    "for sentence in texts:\n",
    "    if n % 100 == 0:\n",
    "        print(n)\n",
    "    doc = nlp(sentence)\n",
    "    AdjInSent = []\n",
    "    totalPos = 0\n",
    "    totalNeg = 0\n",
    "    for token in doc:\n",
    "        if token.text in cachedStopWords:\n",
    "            pass\n",
    "        elif token.is_punct:\n",
    "            pass\n",
    "        elif token.pos_ == 'PRON':\n",
    "            pass\n",
    "        elif token.lemma_ == '-PRON-':\n",
    "            pass\n",
    "        else:\n",
    "            if token.pos_ == 'ADJ':\n",
    "                try:\n",
    "                    breakdown = swn.senti_synset(str(token.lemma_)+'.a.01')\n",
    "                    AdjInSent.append(breakdown)\n",
    "                except:\n",
    "                    pass\n",
    "    for each in AdjInSent:\n",
    "        totalPos = totalPos + each.pos_score()\n",
    "        totalNeg = totalNeg + each.neg_score()\n",
    "    pos_ScoreOfText.append(totalPos)\n",
    "    neg_ScoreOfText.append(totalNeg)\n",
    "    n += 1\n",
    "        \n",
    "   \n",
    "                      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count how many good words /bad words the review contains. Could be a useful feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n"
     ]
    }
   ],
   "source": [
    "#extra all adjectives\n",
    "#combine into string with str(word)+\".a.01\"\n",
    "#store pos_score()  \n",
    "#store neg_score() and add to df\n",
    "pos_ScoreOfTitle = []\n",
    "neg_ScoreOfTitle = []\n",
    "n = 0\n",
    "\n",
    "for sentence in titles:\n",
    "    if n % 100 == 0:\n",
    "        print(n)\n",
    "    doc = nlp(sentence)\n",
    "    AdjInSent = []\n",
    "    totalPos = 0\n",
    "    totalNeg = 0\n",
    "    for token in doc:\n",
    "        if token.text in cachedStopWords:\n",
    "            pass\n",
    "        elif token.is_punct:\n",
    "            pass\n",
    "        elif token.pos_ == 'PRON':\n",
    "            pass\n",
    "        elif token.lemma_ == '-PRON-':\n",
    "            pass\n",
    "        else:\n",
    "            if token.pos_ == 'ADJ':\n",
    "                try:\n",
    "                    breakdown = swn.senti_synset(str(token.lemma_)+'.a.01')\n",
    "                    AdjInSent.append(breakdown)\n",
    "                except:\n",
    "                    pass\n",
    "    for each in AdjInSent:\n",
    "        totalPos = totalPos + each.pos_score()\n",
    "        totalNeg = totalNeg + each.neg_score()\n",
    "    pos_ScoreOfTitle.append(totalPos)\n",
    "    neg_ScoreOfTitle.append(totalNeg)\n",
    "    n += 1\n",
    "        \n",
    "   \n",
    "                      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_words= ['is','baby', 'pack', 'personal', 'using', 'extension', 'changed', 'today', 've', 'kong', 'wider']\n",
    "newText = [] \n",
    "for sentence in texts:\n",
    "    for each in sentence:\n",
    "        if each in remove_words:\n",
    "            sentence = sentence.replace(each, '')\n",
    "    newText.append(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removes 'out of 5 stars' from ratings and converts string to float then an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings2 = []\n",
    "for each in ratings:\n",
    "    score = each.replace(' out of 5 stars', '')\n",
    "    ratings2.append(int(float(score)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "newText = []\n",
    "keep_types = {'NOUN', 'PROPN', 'ADJ', 'VERB'}\n",
    "\n",
    "for sentence in texts:\n",
    "    \n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    keep_tokens_string = ' '.join([t.text for t in doc if t.pos_ in keep_types])\n",
    "    newText.append(keep_tokens_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "cleanTitle = []\n",
    "cleanTitle = [word_tokenize(x) for x in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {'Date': dates, 'Author': authors, 'Title': titles, 'Rating': ratings2, 'Clean Title': cleanTitle, 'Text': newText,'Is Guest': isGuest, 'Review Length (char)': lengthOfText, 'Words in Review': wordsInText, 'Author Name Length': lengthOfAuthor, 'Words In Author Name': wordsInAuthor, 'Cleaned Text': cleanText, 'Positive Sentiment Score of Text': pos_ScoreOfText, 'Negative Sentiment Score of Text': neg_ScoreOfText,'Positive Sentiment Score of Title': pos_ScoreOfTitle,'Negative Sentiment Score of Title': neg_ScoreOfTitle, 'SentimentWords': SentiWords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('outputdata.csv')\n",
    "df.to_json('reviews.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
